{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3757ba",
   "metadata": {},
   "source": [
    "# Part 1 - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeac3dd",
   "metadata": {},
   "source": [
    "Task 1.1 Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f5c33ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC03\n",
      "     day  month  year  Temperature   RH   Ws  Rain   FFMC  DMC    DC  ISI  \\\n",
      "0      1      6  2012           29   57   18    0.0  65.7  3.4   7.6  1.3   \n",
      "1      2      6  2012           29   61   13    1.3  64.4  4.1   7.6  1.0   \n",
      "2      3      6  2012           26   82   22   13.1  47.1  2.5   7.1  0.3   \n",
      "3      4      6  2012           25   89   13    2.5  28.6  1.3   6.9  0.0   \n",
      "4      5      6  2012           27   77   16    0.0  64.8  3.0  14.2  1.2   \n",
      "..   ...    ...   ...          ...  ...  ...    ...   ...  ...   ...  ...   \n",
      "117   26      9  2012           31   54   11    0.0  82.0  6.0  16.3  2.5   \n",
      "118   27      9  2012           31   66   11    0.0  85.7  8.3  24.9  4.0   \n",
      "119   28      9  2012           32   47   14    0.7  77.5  7.1   8.8  1.8   \n",
      "120   29      9  2012           26   80   16    1.8  47.4  2.9   7.7  0.3   \n",
      "121   30      9  2012           25   78   14    1.4  45.0  1.9   7.5  0.2   \n",
      "\n",
      "     BUI  FWI  \n",
      "0    3.4  0.5  \n",
      "1    3.9  0.4  \n",
      "2    2.7  0.1  \n",
      "3    1.7  0.0  \n",
      "4    3.9  0.5  \n",
      "..   ...  ...  \n",
      "117  6.2  1.7  \n",
      "118  9.0  4.1  \n",
      "119  6.8  0.9  \n",
      "120  3.0  0.1  \n",
      "121  2.4  0.1  \n",
      "\n",
      "[122 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "df1 = pd.read_csv('AlgerianFF_Region1.csv')\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "print (df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf8d090",
   "metadata": {},
   "source": [
    "Splitting data for dataset into inputs & targets for Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b13f3752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 6.000e+00 2.012e+03 ... 7.600e+00 1.300e+00 3.400e+00]\n",
      " [2.000e+00 6.000e+00 2.012e+03 ... 7.600e+00 1.000e+00 3.900e+00]\n",
      " [3.000e+00 6.000e+00 2.012e+03 ... 7.100e+00 3.000e-01 2.700e+00]\n",
      " ...\n",
      " [2.800e+01 9.000e+00 2.012e+03 ... 8.800e+00 1.800e+00 6.800e+00]\n",
      " [2.900e+01 9.000e+00 2.012e+03 ... 7.700e+00 3.000e-01 3.000e+00]\n",
      " [3.000e+01 9.000e+00 2.012e+03 ... 7.500e+00 2.000e-01 2.400e+00]] [ 0.5  0.4  0.1  0.   0.5  2.5  7.2  7.1  0.3  0.9  5.6  7.1  0.2  0.4\n",
      "  0.1  0.   0.   0.2  1.4  0.4  2.2  2.3  3.8  7.5  8.4 10.6 15.  13.9\n",
      "  3.9 12.9  0.4  0.3  0.5  1.7  4.9  6.8  3.2  8.   0.6  0.5  2.2  0.9\n",
      "  3.4  0.8  0.5  0.4  3.6  6.  10.9  4.   8.8  2.8  2.1  1.3  7.3 15.3\n",
      " 15.3 11.3 11.9 10.7 15.7  0.9  0.8  0.8  3.9  6.1  6.8  8.   2.6  9.9\n",
      " 11.6 12.1  4.2 10.2 10.6  6.3  4.2 14.6 16.1 17.2 16.8 18.4 20.4 22.3\n",
      " 20.9 20.3 13.7 13.2 19.9 30.2  4.2  0.3  0.1  0.   0.   0.5  1.7  0.9\n",
      "  0.6  0.4  0.2  0.3  0.3  0.5  0.   0.1  0.   0.8  5.9  7.7  9.7  6.3\n",
      "  8.3  2.8  0.7  0.7  0.1  1.7  4.1  0.9  0.1  0.1]\n"
     ]
    }
   ],
   "source": [
    "df1_inputs = df1.values[:,:-1].astype(float)\n",
    "df1_targets = df1.values[:,-1].astype(float)\n",
    "print(df1_inputs, df1_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "88a8c5ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-44a0dd7f347a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'AlgerianFF_Region2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pandas' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_csv ('AlgerianFF_Region2.csv')\n",
    "data = pandas.concat([df1, df2])\n",
    "print (df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b12a2",
   "metadata": {},
   "source": [
    "Splitting data for dataset into inputs & targets for Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a58b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Second Dataset \\n\")\n",
    "df2_inputs = df2.values[:,:-1].astype(float)\n",
    "df2_targets = df2.values[:,-1].astype(float)\n",
    "print(df1_inputs, df1_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "03123344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     day  month  year  Temperature   RH   Ws  Rain   FFMC   DMC    DC  ISI  \\\n",
      "0      1      6  2012           29   57   18    0.0  65.7   3.4   7.6  1.3   \n",
      "1      2      6  2012           29   61   13    1.3  64.4   4.1   7.6  1.0   \n",
      "2      3      6  2012           26   82   22   13.1  47.1   2.5   7.1  0.3   \n",
      "3      4      6  2012           25   89   13    2.5  28.6   1.3   6.9  0.0   \n",
      "4      5      6  2012           27   77   16    0.0  64.8   3.0  14.2  1.2   \n",
      "..   ...    ...   ...          ...  ...  ...    ...   ...   ...   ...  ...   \n",
      "117   26      9  2012           30   65   14    0.0  85.4  16.0  44.5  4.5   \n",
      "118   27      9  2012           28   87   15    4.4  41.1   6.5   8.0  0.1   \n",
      "119   28      9  2012           27   87   29    0.5  45.9   3.5   7.9  0.4   \n",
      "120   29      9  2012           24   54   18    0.1  79.7   4.3  15.2  1.7   \n",
      "121   30      9  2012           24   64   15    0.2  67.3   3.8  16.5  1.2   \n",
      "\n",
      "      BUI  FWI  \n",
      "0     3.4  0.5  \n",
      "1     3.9  0.4  \n",
      "2     2.7  0.1  \n",
      "3     1.7  0.0  \n",
      "4     3.9  0.5  \n",
      "..    ...  ...  \n",
      "117  16.9  6.5  \n",
      "118   6.2  0.0  \n",
      "119   3.4  0.2  \n",
      "120   5.1  0.7  \n",
      "121   4.8  0.5  \n",
      "\n",
      "[244 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#combine df1 (dataset1) with df2 (dataset2)\n",
    "output_data = df1.append(df2)\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da125f",
   "metadata": {},
   "source": [
    "Noramlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a283e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "df1_inputs = preprocessing.normalize(df1_inputs)\n",
    "#print(df1_inputs)\n",
    "\n",
    "\n",
    "#Scale the inputs\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df1_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_inputs = preprocessing.normalize(df2_inputs)\n",
    "#print(df1_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d02210",
   "metadata": {},
   "source": [
    "Task 1.2 Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d35ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d03ae63",
   "metadata": {},
   "source": [
    "a) Creation of regression tool for MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor1 = MLPRegressor(solver='lbfgs', max_iter=4000)\n",
    "regressor1.fit(df1, df1_targets)\n",
    "outputs = regressor1.predict(df1)\n",
    "print(\"Mean absolute error: \", mean_absolute_error(df1_targets, outputs))\n",
    "\n",
    "#calculate MAE\n",
    "REG1_mae = abs(cross_val_score(regressor1, df1_inputs, df1_targets, scoring=\"neg_mean_absolute_error\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e677226",
   "metadata": {},
   "source": [
    "b) Creation of regression tool for SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2da795",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVRregressor = SVR()\n",
    "SVRregressor.fit(df1_inputs, df1_targets)\n",
    "SVRoutputs = SVRregressor.predict(df1_inputs)\n",
    "\n",
    "print(\"Mean absolute error: \", mean_absolute_error(df1_targets, SVRoutputs))\n",
    "\n",
    "#calculate MAE\n",
    "RFreg_mae = abs(cross_val_score(SVRregressor, df1_inputs, df1_targets, scoring=\"neg_mean_absolute_error\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2572b4c",
   "metadata": {},
   "source": [
    "c) Creation of regressor tool for RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandFor_Regressor = RandomForestRegressor()\n",
    "RandFor_Regressor.fit(df1_inputs, df1_targets)\n",
    "outputs =  RandFor_Regressor.predict(df1_inputs)\n",
    "\n",
    "print(\"Mean absolute error: \", mean_absolute_error(df1_targets, outputs))\n",
    "\n",
    "#calculate MAE\n",
    "RandREG_mae = abs(cross_val_score(RandFor_Regressor, df1_inputs, df1_targets, scoring=\"neg_mean_absolute_error\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211b0d7",
   "metadata": {},
   "source": [
    "Task 1.3 Assessment of regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd882bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store mae results into a python array \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "maeArray = [REG1_mae, RFreg_mae, RandREG_mae]\n",
    "\n",
    "REG1 = np.random.rand(100)\n",
    "REG2 = np.random.randn(100)\n",
    "REG3 = np.random.rand(100)\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot([REG1_mae, RFreg_mae, RandREG_mae])\n",
    "plt.xticks([1,2,3], [\"MLPRegressor\", \"SVRRegressor\", \"RandomForestRegressor\"]);\n",
    "plt.ylabel(\"$p(x)$\")\n",
    "plt.savefig(\"boxplot.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f185c517",
   "metadata": {},
   "source": [
    "# PART 2 – EVOLUTIONARY COMPUTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183198bb",
   "metadata": {},
   "source": [
    "Task 2.1 –Generation of random solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pd.read_csv(r'distances.csv', header=None)\n",
    "print (distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = open(r'cities.txt')\n",
    "test = cities.read().splitlines()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae1a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# dislaying distances\n",
    "\n",
    "def randRoute(distances):\n",
    "    route = list(range(len(distances)))\n",
    "    #shuffeling cities\n",
    "    random.shuffle(route)\n",
    "    return route\n",
    "print(randRoute(distances)) # creating a random route\n",
    "\n",
    "\n",
    "#working out length of route\n",
    "\n",
    "def route_length(route, dist):\n",
    "    length = 0 #sets the varaible 'length' to zero\n",
    "    for i in range(len(route)): # looks through the array\n",
    "       length += dist.iat[route[i-1], route[i]] # for how many cities are in the route, get distance of first city and next city\n",
    "    \n",
    "    return length\n",
    "\n",
    "x = randRoute(distances)\n",
    "y = route_length(x, distances)\n",
    "print(x)\n",
    "print(y)\n",
    "#x = randRoute(distances)\n",
    "#print (x)\n",
    "#y = route_length(x, distances)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e723e77",
   "metadata": {},
   "source": [
    "Task 2.2 –Algorithm implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72919280",
   "metadata": {},
   "source": [
    "2.2.1 - Creation of Ruin and Recreate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fecd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruinAndRecreate(iterations):\n",
    "    \n",
    "    RandList = []\n",
    "    \n",
    "    bestRoute = randRoute(distances)\n",
    "    bestRouteLength = route_length(bestRoute, distances)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        route2 = randRoute(distances)\n",
    "        print(route2)\n",
    "        routelength2 = route_length(route2, distances)\n",
    "        if routelength2 < bestRouteLength:\n",
    "            bestRouteLength = routelength2\n",
    "            bestRoute = route2\n",
    "        RandList.append(bestRouteLength) #iterates through and generates a new route , if it was better than previous routes it becomes the best route is kept till the next iteration \n",
    "    return RandList\n",
    "#print(ruinAndRecreate(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3318f5f",
   "metadata": {},
   "source": [
    "2.2.1 - Creation of swap function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5986496f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def swap(iterations):\n",
    "    print(\"swap Function\") # for readability\n",
    "    \n",
    "    SwapList = []\n",
    "    \n",
    "    bestRoute = randRoute(distances)\n",
    "    bestRouteLength = route_length(bestRoute, distances)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        newRoute = bestRoute.copy() # copies the values of bestRoute\n",
    "        newRouteLength = route_length(newRoute, distances) #records the length of the new route, and gives something to compare what route is more fitter out of the bestroute and newroute\n",
    "        \n",
    "        print(\"Best route = \", bestRoute, \": Length = \", bestRouteLength) # make sure it works\n",
    "        \n",
    "        newRoutePosition1 = random.randint(0, max(newRoute))\n",
    "        newRoutePosition2 = random.randint(0, max(newRoute))\n",
    "        \n",
    "        \n",
    "        newRoute[newRoutePosition1], newRoute[newRoutePosition2] = newRoute[newRoutePosition2], newRoute[newRoutePosition1]\n",
    "        print(\"New Route = \", newRoute, \": Length = \", newRouteLength)\n",
    "        newRouteLength = route_length(newRoute, distances)\n",
    "        \n",
    "        if(bestRouteLength > newRouteLength):\n",
    "            bestRoute = newRoute\n",
    "            bestRouteLength = newRouteLength\n",
    "        else:\n",
    "            bestRoute = bestRoute\n",
    "            bestRouteLength = bestRouteLength\n",
    "        SwapList.append(bestRouteLength)\n",
    "        print(\"Best Route = \", bestRoute, \": Length = \", bestRouteLength)\n",
    "        \n",
    "    print(\"Best Route = \", route_length(bestRoute, distances))\n",
    "        \n",
    "    \n",
    "    return SwapList\n",
    "\n",
    "#print(swap(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e3419",
   "metadata": {},
   "source": [
    "Task 2.3 – Visualisation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d977f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def results():\n",
    "    SwapList = []\n",
    "    RuinNRecreateList = []\n",
    "    \n",
    "    for i in range(30):\n",
    "        SwapList.append(swap(30))\n",
    "        RuinNRecreateList.append(ruinAndRecreate(30))\n",
    "        \n",
    "        # turning both lists into numpy arrays because it works out max , min and average calculations\n",
    "        Array1 = numpy.array(SwapList) \n",
    "        Array2 = numpy.array(RuinNRecreateList)\n",
    "\n",
    "    plt.plot(Array1.min(axis=0))\n",
    "    plt.plot(Array1.max(axis=0))\n",
    "    plt.plot(Array1.mean(axis=0))\n",
    "    \n",
    "    plt.plot(Array2.min(axis=0))\n",
    "    plt.plot(Array2.max(axis=0))\n",
    "    plt.plot(Array2.mean(axis=0))\n",
    "    plt.legend([\"Swap mutation minimum\",\"Swap mutation maximum\", \"Swap mutation mean\", \"Ruin and Recreate minimum\", \"Ruin and Recreate maximum\", \"Ruin and Recreate mean\"], loc=(1,0.5))\n",
    "    plt.show()\n",
    "    \n",
    "results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e267b",
   "metadata": {},
   "source": [
    "The best mutation operator for the traveling saleman problem is the swap function because on the graph the mean value is seen traveling downwards at a faster and more reliable rate , where as the Ruin and recreate is slower. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
